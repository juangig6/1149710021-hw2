#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Part A: å‚³çµ±æ–¹æ³•
åŒ…å« A-1, A-2, A-3 ä¸‰å€‹ä»»å‹™
"""
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import os
import sys
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams
from wordcloud import WordCloud

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================
# A-1: TF-IDF æ–‡æœ¬ç›¸ä¼¼åº¦è¨ˆç®—
# ============================================================
def run_a1():
    """A-1: TF-IDF æ–‡æœ¬ç›¸ä¼¼åº¦è¨ˆç®—"""
    print("\nA-1: TF-IDF æ–‡æœ¬ç›¸ä¼¼åº¦è¨ˆç®—")
    print("="*60)

    # æ¸¬è©¦æ–‡æœ¬
    documents = [
        "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œï¼Œæ©Ÿå™¨å­¸ç¿’æ˜¯å…¶æ ¸å¿ƒæŠ€è¡“",
        "æ·±åº¦å­¸ç¿’æ¨å‹•äº†äººå·¥æ™ºæ…§çš„ç™¼å±•ï¼Œç‰¹åˆ¥æ˜¯åœ¨åœ–åƒè­˜åˆ¥é ˜åŸŸ",
        "ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œé©åˆå‡ºå»é‹å‹•",
        "æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’éƒ½æ˜¯äººå·¥æ™ºæ…§çš„é‡è¦åˆ†æ”¯",
        "é‹å‹•æœ‰ç›Šå¥åº·ï¼Œæ¯å¤©éƒ½æ‡‰è©²ä¿æŒé‹å‹•ç¿’æ…£"
    ]

    print("æ¸¬è©¦æ–‡æª”ï¼š")
    for i, doc in enumerate(documents, 1):
        print(f"{i}. {doc}")

    # ä¸­æ–‡åˆ†è©
    def tokenize(text):
        return " ".join(jieba.cut(text))

    print("\n" + "="*50)
    print("æ­¥é©Ÿ 1: ä¸­æ–‡åˆ†è©çµæœ")
    print("="*50)
    tokenized_docs = [tokenize(doc) for doc in documents]
    for i, doc in enumerate(tokenized_docs, 1):
        print(f"æ–‡æª” {i}: {doc}")

    # TF-IDF è¨ˆç®—
    print("\n" + "="*50)
    print("æ­¥é©Ÿ 2: TF-IDF è¨ˆç®—")
    print("="*50)
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(tokenized_docs)

    # å–å¾—ç‰¹å¾µè©å½™
    feature_names = vectorizer.get_feature_names_out()
    print(f"\nè©å½™è¡¨å¤§å°: {len(feature_names)} å€‹è©")
    # print(f"è©å½™è¡¨: {', '.join(feature_names[:20])}{'...' if len(feature_names) > 20 else ''}")
    print(f"è©å½™è¡¨: {', '.join(feature_names)}")

    # è¨ˆç®— IDF å€¼
    idf_values = dict(zip(feature_names, vectorizer.idf_))

    # é¡¯ç¤ºæ¯å€‹æ–‡æª”çš„ TF-IDF å€¼
    print("\n" + "="*50)
    print("æ­¥é©Ÿ 3: å„æ–‡æª”çš„ TF-IDF å€¼è©³ç´°åˆ†æ")
    print("="*50)

    for doc_idx in range(len(documents)):
        print(f"\nã€æ–‡æª” {doc_idx + 1}ã€‘: {documents[doc_idx]}")
        print("-" * 50)
        
        tfidf_vector = tfidf_matrix[doc_idx].toarray()[0]
        word_scores = [(feature_names[i], tfidf_vector[i]) 
                       for i in range(len(feature_names)) 
                       if tfidf_vector[i] > 0]
        word_scores.sort(key=lambda x: x[1], reverse=True)
        
        doc_words = tokenized_docs[doc_idx].split()
        word_count = {}
        for word in doc_words:
            word_count[word] = word_count.get(word, 0) + 1
        total_words = len(doc_words)
        
        if doc_idx == 0:
            print("\nğŸ“Š Top 10 é—œéµè© (å« TF, IDF, TF-IDF è©³ç´°è¨ˆç®—):")
            print(f"{'æ’å':<4} {'è©å½™':<10} {'TF':<10} {'IDF':<10} {'TF-IDF':<10}")
            print("-" * 60)
            
            for rank, (word, tfidf_score) in enumerate(word_scores[:10], 1):
                tf = word_count.get(word, 0) / total_words
                idf = idf_values[word]
                print(f"{rank:<4} {word:<10} {tf:<10.4f} {idf:<10.4f} {tfidf_score:<10.4f}")
            
            if len(word_scores) > 10:
                print(f"\n  ... (é‚„æœ‰ {len(word_scores) - 10} å€‹è©)")
            
            print("\n" + "="*60)
            print("ğŸ“ è¨ˆç®—èªªæ˜:")
            print("="*60)
            print("TF (Term Frequency, è©é »):")
            print("  å…¬å¼: TF = è©²è©åœ¨æ–‡æª”ä¸­å‡ºç¾æ¬¡æ•¸ / æ–‡æª”ç¸½è©æ•¸")
            print(f"  ç¯„ä¾‹: '{word_scores[0][0]}' åœ¨æ–‡æª”1ä¸­å‡ºç¾ {word_count.get(word_scores[0][0], 0)} æ¬¡")
            print(f"       æ–‡æª”1ç¸½å…±æœ‰ {total_words} å€‹è©")
            print(f"       TF = {word_count.get(word_scores[0][0], 0)}/{total_words} = {word_count.get(word_scores[0][0], 0)/total_words:.4f}")
            
            print("\nIDF (Inverse Document Frequency, é€†æ–‡æª”é »ç‡):")
            print("  å…¬å¼: IDF = log(æ–‡æª”ç¸½æ•¸ / åŒ…å«è©²è©çš„æ–‡æª”æ•¸)")
            
            first_word = word_scores[0][0]
            doc_containing_word = sum(1 for doc in tokenized_docs if first_word in doc.split())
            print(f"  ç¯„ä¾‹: '{first_word}' å‡ºç¾åœ¨ {doc_containing_word} å€‹æ–‡æª”ä¸­")
            print(f"       ç¸½å…±æœ‰ {len(documents)} å€‹æ–‡æª”")
            print(f"       IDF = log({len(documents)}/{doc_containing_word}) = {idf_values[first_word]:.4f}")
            
            print("\nTF-IDF (Term Frequency-Inverse Document Frequency):")
            print("  å…¬å¼: TF-IDF = TF Ã— IDF")
            tf_first = word_count.get(first_word, 0) / total_words
            print(f"  ç¯„ä¾‹: '{first_word}' çš„ TF-IDF")
            print(f"       = {tf_first:.4f} Ã— {idf_values[first_word]:.4f}")
            print(f"       = {word_scores[0][1]:.4f}")
            
            print("\nğŸ’¡ è§£è®€:")
            print("  â€¢ TF è¶Šé«˜ = è©²è©åœ¨æ­¤æ–‡æª”ä¸­è¶Šé‡è¦")
            print("  â€¢ IDF è¶Šé«˜ = è©²è©åœ¨æ•´å€‹æ–‡æª”é›†ä¸­è¶Šç½•è¦‹,è¶Šæœ‰å€åˆ¥æ€§")
            print("  â€¢ TF-IDF è¶Šé«˜ = è©²è©æ˜¯æ­¤æ–‡æª”çš„é—œéµç‰¹å¾µè©")
            print("="*60)
        else:
            print("\nTop 10 é—œéµè© (æŒ‰ TF-IDF åˆ†æ•¸æ’åº):")
            print(f"{'æ’å':<4} {'è©å½™':<10} {'TF-IDF':<10}")
            print("-" * 30)
            for rank, (word, score) in enumerate(word_scores[:10], 1):
                print(f"{rank:<4} {word:<10} {score:<10.4f}")
            
            if len(word_scores) > 10:
                print(f"\n  ... (é‚„æœ‰ {len(word_scores) - 10} å€‹è©)")
            
            if doc_idx == 1:
                print("\nè¨»: æ–‡æª” 2-5 çš„ TF, IDF è¨ˆç®—æ–¹å¼ç›¸åŒ,ä¸å†è´…è¿°ã€‚")

    # å»ºç«‹å®Œæ•´çš„ TF-IDF DataFrame
    print("\n" + "="*50)
    print("æ­¥é©Ÿ 4: TF-IDF çŸ©é™£ (å®Œæ•´)")
    print("="*50)
    tfidf_df = pd.DataFrame(
        tfidf_matrix.toarray(),
        columns=feature_names,
        index=[f"æ–‡æª”{i+1}" for i in range(len(documents))]
    )

    tfidf_df_nonzero = tfidf_df.loc[:, (tfidf_df != 0).any(axis=0)]
    print(f"\nå®Œæ•´ TF-IDF çŸ©é™£ (é¡¯ç¤ºå‰ 10 å€‹è©å½™):")
    print(tfidf_df_nonzero.iloc[:, :10].round(4))
    if tfidf_df_nonzero.shape[1] > 10:
        print(f"... (é‚„æœ‰ {tfidf_df_nonzero.shape[1] - 10} å€‹è©)")

    # è¨ˆç®—ç›¸ä¼¼åº¦
    print("\n" + "="*50)
    print("æ­¥é©Ÿ 5: æ–‡æª”ç›¸ä¼¼åº¦è¨ˆç®— (Cosine Similarity)")
    print("="*50)
    similarity = cosine_similarity(tfidf_matrix)

    similarity_df = pd.DataFrame(
        similarity,
        columns=[f"æ–‡æª”{i+1}" for i in range(len(documents))],
        index=[f"æ–‡æª”{i+1}" for i in range(len(documents))]
    )

    print("\nç›¸ä¼¼åº¦çŸ©é™£:")
    print(similarity_df.round(4))

    print("\n" + "-"*50)
    print("æ–‡æª”é–“ç›¸ä¼¼åº¦åˆ†æ:")
    print("-"*50)

    similar_pairs = []
    for i in range(len(documents)):
        for j in range(i+1, len(documents)):
            similar_pairs.append((i+1, j+1, similarity[i][j]))

    similar_pairs.sort(key=lambda x: x[2], reverse=True)

    print("\næœ€ç›¸ä¼¼çš„æ–‡æª”å° (Top 5):")
    for rank, (doc1, doc2, score) in enumerate(similar_pairs[:5], 1):
        print(f"\n{rank}. æ–‡æª”{doc1} â†” æ–‡æª”{doc2} : ç›¸ä¼¼åº¦ = {score:.4f}")
        print(f"   æ–‡æª”{doc1}: {documents[doc1-1]}")
        print(f"   æ–‡æª”{doc2}: {documents[doc2-1]}")

    print("\næœ€ä¸ç›¸ä¼¼çš„æ–‡æª”å° (Top 3):")
    for rank, (doc1, doc2, score) in enumerate(similar_pairs[-3:], 1):
        print(f"\n{rank}. æ–‡æª”{doc1} â†” æ–‡æª”{doc2} : ç›¸ä¼¼åº¦ = {score:.4f}")
        print(f"   æ–‡æª”{doc1}: {documents[doc1-1]}")
        print(f"   æ–‡æª”{doc2}: {documents[doc2-1]}")

    # å„²å­˜çµæœ
    os.makedirs('results', exist_ok=True)
    tfidf_df.to_csv('results/a1_tfidf_matrix.csv', encoding='utf-8-sig')
    similarity_df.to_csv('results/a1_similarity_matrix.csv', encoding='utf-8-sig')
    
    with open('results/a1_top_keywords.txt', 'w', encoding='utf-8') as f:
        for doc_idx in range(len(documents)):
            f.write(f"æ–‡æª” {doc_idx + 1}: {documents[doc_idx]}\n")
            f.write("-" * 50 + "\n")
            tfidf_vector = tfidf_matrix[doc_idx].toarray()[0]
            word_scores = [(feature_names[i], tfidf_vector[i]) 
                          for i in range(len(feature_names)) 
                          if tfidf_vector[i] > 0]
            word_scores.sort(key=lambda x: x[1], reverse=True)
            for rank, (word, score) in enumerate(word_scores[:10], 1):
                f.write(f"{rank:2d}. {word:8s} : {score:.4f}\n")
            f.write("\n")

    print("\nâœ“ A-1 å®Œæˆï¼")


# ============================================================
# A-2: åŸºæ–¼è¦å‰‡çš„æ–‡æœ¬åˆ†é¡
# ============================================================
def run_a2():
    """A-2: åŸºæ–¼è¦å‰‡çš„æ–‡æœ¬åˆ†é¡"""
    print("\n\nA-2: åŸºæ–¼è¦å‰‡çš„æ–‡æœ¬åˆ†é¡ (15 åˆ†)")
    print("="*60)

    # 1. æƒ…æ„Ÿåˆ†é¡å™¨
    print("\n1. æƒ…æ„Ÿåˆ†é¡å™¨ (8 åˆ†)")
    print("-"*60)

    class RuleBasedSentimentClassifier:
        def __init__(self):
            # å»ºç«‹æ­£è² é¢è©å½™åº«
            self.positive_words = ['å¥½', 'æ£’', 'å„ªç§€', 'å–œæ­¡', 'æ¨è–¦',
                                  'æ»¿æ„', 'é–‹å¿ƒ', 'å€¼å¾—', 'ç²¾å½©', 'å®Œç¾']
            self.negative_words = ['å·®', 'ç³Ÿ', 'å¤±æœ›', 'è¨å­', 'ä¸æ¨è–¦',
                                  'æµªè²»', 'ç„¡èŠ', 'çˆ›', 'ç³Ÿç³•', 'å·®å‹']
            
            # åŠ å…¥å¦å®šè©è™•ç†
            self.negation_words = ['ä¸', 'æ²’', 'ç„¡', 'é', 'åˆ¥']
        
        def classify(self, text):
            words = list(jieba.cut(text))
            positive_count = 0
            negative_count = 0
            
            for i, word in enumerate(words):
                has_negation = False
                if i > 0 and words[i-1] in self.negation_words:
                    has_negation = True
                
                if word in self.positive_words:
                    if has_negation:
                        negative_count += 1
                    else:
                        positive_count += 1
                elif word in self.negative_words:
                    if has_negation:
                        positive_count += 1
                    else:
                        negative_count += 1
            
            if positive_count > negative_count:
                sentiment = "æ­£é¢"
                confidence = positive_count / (positive_count + negative_count) if (positive_count + negative_count) > 0 else 0
            elif negative_count > positive_count:
                sentiment = "è² é¢"
                confidence = negative_count / (positive_count + negative_count) if (positive_count + negative_count) > 0 else 0
            else:
                sentiment = "ä¸­æ€§"
                confidence = 0.5
            
            return {
                'sentiment': sentiment,
                'confidence': confidence,
                'positive_count': positive_count,
                'negative_count': negative_count,
                'words': words
            }

    # 2. ä¸»é¡Œåˆ†é¡å™¨
    print("\n2. ä¸»é¡Œåˆ†é¡å™¨ (7 åˆ†)")
    print("-"*60)

    class TopicClassifier:
        def __init__(self):
            self.topic_keywords = {
                'ç§‘æŠ€': ['AI', 'äººå·¥æ™ºæ…§', 'é›»è…¦', 'è»Ÿé«”', 'ç¨‹å¼', 'æ¼”ç®—æ³•'],
                'é‹å‹•': ['é‹å‹•', 'å¥èº«', 'è·‘æ­¥', 'æ¸¸æ³³', 'çƒé¡', 'æ¯”è³½'],
                'ç¾é£Ÿ': ['åƒ', 'é£Ÿç‰©', 'é¤å»³', 'ç¾å‘³', 'æ–™ç†', 'çƒ¹é£ª'],
                'æ—…éŠ': ['æ—…è¡Œ', 'æ™¯é»', 'é£¯åº—', 'æ©Ÿç¥¨', 'è§€å…‰', 'åº¦å‡']
            }
        
        def classify(self, text):
            words = set(jieba.cut(text))
            topic_scores = {}
            for topic, keywords in self.topic_keywords.items():
                matches = sum(1 for keyword in keywords if keyword in words)
                topic_scores[topic] = matches
            
            if max(topic_scores.values()) > 0:
                best_topic = max(topic_scores, key=topic_scores.get)
                return {
                    'topic': best_topic,
                    'scores': topic_scores,
                    'confidence': topic_scores[best_topic] / sum(topic_scores.values())
                }
            else:
                return {
                    'topic': 'å…¶ä»–',
                    'scores': topic_scores,
                    'confidence': 0
                }

    # æ¸¬è©¦è³‡æ–™
    test_texts = [
        "é€™å®¶é¤å»³çš„ç‰›è‚‰éºµçœŸçš„å¤ªå¥½åƒäº†ï¼Œæ¹¯é ­æ¿ƒéƒï¼Œéºµæ¢Qå½ˆï¼Œä¸‹æ¬¡ä¸€å®šå†ä¾†ï¼",
        "æœ€æ–°çš„AIæŠ€è¡“çªç ´è®“äººé©šè‰·ï¼Œæ·±åº¦å­¸ç¿’æ¨¡å‹çš„è¡¨ç¾è¶Šä¾†è¶Šå¥½",
        "é€™éƒ¨é›»å½±åŠ‡æƒ…ç©ºæ´ï¼Œæ¼”æŠ€ç³Ÿç³•ï¼Œå®Œå…¨æ˜¯æµªè²»æ™‚é–“",
        "æ¯å¤©æ…¢è·‘5å…¬é‡Œï¼Œé…åˆé©ç•¶çš„é‡è¨“ï¼Œé«”èƒ½é€²æ­¥å¾ˆå¤š"
    ]

    print("\næ¸¬è©¦æ–‡æœ¬:")
    for i, text in enumerate(test_texts, 1):
        print(f"{i}. {text}")

    # æ¸¬è©¦æƒ…æ„Ÿåˆ†é¡å™¨
    print("\n" + "="*60)
    print("æƒ…æ„Ÿåˆ†é¡çµæœ:")
    print("="*60)

    sentiment_classifier = RuleBasedSentimentClassifier()
    for i, text in enumerate(test_texts, 1):
        result = sentiment_classifier.classify(text)
        print(f"\næ–‡æœ¬ {i}: {text}")
        print(f"  æƒ…æ„Ÿ: {result['sentiment']}")
        print(f"  ä¿¡å¿ƒåº¦: {result['confidence']:.2%}")
        print(f"  æ­£é¢è©æ•¸: {result['positive_count']}, è² é¢è©æ•¸: {result['negative_count']}")
        print(f"  åˆ†è©çµæœ: {' / '.join(result['words'][:15])}{'...' if len(result['words']) > 15 else ''}")

    # æ¸¬è©¦ä¸»é¡Œåˆ†é¡å™¨
    print("\n" + "="*60)
    print("ä¸»é¡Œåˆ†é¡çµæœ:")
    print("="*60)

    topic_classifier = TopicClassifier()
    for i, text in enumerate(test_texts, 1):
        result = topic_classifier.classify(text)
        print(f"\næ–‡æœ¬ {i}: {text}")
        print(f"  ä¸»é¡Œ: {result['topic']}")
        print(f"  ä¿¡å¿ƒåº¦: {result['confidence']:.2%}")
        print(f"  å„ä¸»é¡Œåˆ†æ•¸: {result['scores']}")

    # å„²å­˜çµæœ
    os.makedirs('results', exist_ok=True)
    with open('results/a2_classification_results.txt', 'w', encoding='utf-8') as f:
        f.write("A-2: åŸºæ–¼è¦å‰‡çš„æ–‡æœ¬åˆ†é¡çµæœ\n")
        f.write("="*60 + "\n\n")
        f.write("æƒ…æ„Ÿåˆ†é¡çµæœ:\n")
        f.write("-"*60 + "\n")
        for i, text in enumerate(test_texts, 1):
            result = sentiment_classifier.classify(text)
            f.write(f"\næ–‡æœ¬ {i}: {text}\n")
            f.write(f"  æƒ…æ„Ÿ: {result['sentiment']}\n")
            f.write(f"  ä¿¡å¿ƒåº¦: {result['confidence']:.2%}\n")
            f.write(f"  æ­£é¢è©æ•¸: {result['positive_count']}, è² é¢è©æ•¸: {result['negative_count']}\n\n")
        
        f.write("\n" + "="*60 + "\n")
        f.write("ä¸»é¡Œåˆ†é¡çµæœ:\n")
        f.write("-"*60 + "\n")
        for i, text in enumerate(test_texts, 1):
            result = topic_classifier.classify(text)
            f.write(f"\næ–‡æœ¬ {i}: {text}\n")
            f.write(f"  ä¸»é¡Œ: {result['topic']}\n")
            f.write(f"  ä¿¡å¿ƒåº¦: {result['confidence']:.2%}\n")
            f.write(f"  å„ä¸»é¡Œåˆ†æ•¸: {result['scores']}\n\n")

    print("\nâœ“ A-2 å®Œæˆï¼")


# ============================================================
# A-3: çµ±è¨ˆå¼è‡ªå‹•æ‘˜è¦
# ============================================================
def run_a3():
    """A-3: çµ±è¨ˆå¼è‡ªå‹•æ‘˜è¦"""
    print("\n\nA-3: çµ±è¨ˆå¼è‡ªå‹•æ‘˜è¦ (15 åˆ†)")
    print("="*60)

    class StatisticalSummarizer:
        def __init__(self):
            # è¼‰å…¥åœç”¨è©
            self.stop_words = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ',
                                  'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹', 'ä¸Š',
                                  'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ '])
        
        def sentence_score(self, sentence, word_freq):
            """
            è¨ˆç®—å¥å­é‡è¦æ€§åˆ†æ•¸
            è€ƒæ…®å› ç´ :
            1. åŒ…å«é«˜é »è©çš„æ•¸é‡
            2. å¥å­ä½ç½® (é¦–å°¾å¥åŠ æ¬Š)
            3. å¥å­é•·åº¦ (å¤ªçŸ­æˆ–å¤ªé•·æ‰£åˆ†)
            4. æ˜¯å¦åŒ…å«æ•¸å­—æˆ–å°ˆæœ‰åè©
            """
            # åˆ†è©ä¸¦éæ¿¾åœç”¨è©
            words = [w for w in jieba.cut(sentence) if w not in self.stop_words and len(w) > 1]
            
            if len(words) == 0:
                return 0
            
            # 1. è¨ˆç®—é«˜é »è©åˆ†æ•¸
            word_score = sum(word_freq.get(word, 0) for word in words) / len(words)
            
            # 2. å¥å­é•·åº¦åˆ†æ•¸ (åå¥½ä¸­ç­‰é•·åº¦å¥å­)
            length = len(sentence)
            if length < 10:
                length_score = 0.5
            elif length > 100:
                length_score = 0.7
            else:
                length_score = 1.0
            
            # 3. æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­—
            has_number = any(char.isdigit() for char in sentence)
            number_score = 1.2 if has_number else 1.0
            
            # ç¶œåˆåˆ†æ•¸
            final_score = word_score * length_score * number_score
            
            return final_score
        
        def summarize(self, text, ratio=0.3):
            """
            ç”Ÿæˆæ‘˜è¦æ­¥é©Ÿ:
            1. åˆ†å¥ (è™•ç†ä¸­æ–‡æ¨™é»)
            2. åˆ†è©ä¸¦è¨ˆç®—è©é »
            3. è¨ˆç®—æ¯å€‹å¥å­çš„é‡è¦æ€§åˆ†æ•¸
            4. é¸æ“‡æœ€é«˜åˆ†çš„å¥å­
            5. æŒ‰åŸæ–‡é †åºæ’åˆ—
            """
            # 1. åˆ†å¥ (è™•ç†ä¸­æ–‡æ¨™é»)
            import re
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            print(f"\nåŸæ–‡å…± {len(sentences)} å€‹å¥å­")
            print("-" * 60)
            for i, sent in enumerate(sentences, 1):
                print(f"{i}. {sent}")
            
            # 2. åˆ†è©ä¸¦è¨ˆç®—è©é »
            print("\n" + "="*60)
            print("æ­¥é©Ÿ 1: åˆ†è©ä¸¦è¨ˆç®—è©é »")
            print("="*60)
            
            all_words = []
            for sentence in sentences:
                words = [w for w in jieba.cut(sentence) 
                        if w not in self.stop_words and len(w) > 1]
                all_words.extend(words)
            
            # è¨ˆç®—è©é »
            word_freq = {}
            for word in all_words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # é¡¯ç¤º Top 15 é«˜é »è©
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            print(f"\nTop 15 é«˜é »è©:")
            for i, (word, freq) in enumerate(sorted_words[:15], 1):
                print(f"  {i:2d}. {word:8s} : {freq} æ¬¡")
            
            # 3. è¨ˆç®—æ¯å€‹å¥å­çš„é‡è¦æ€§åˆ†æ•¸
            print("\n" + "="*60)
            print("æ­¥é©Ÿ 2: è¨ˆç®—å¥å­é‡è¦æ€§åˆ†æ•¸")
            print("="*60)
            
            sentence_scores = []
            for i, sentence in enumerate(sentences):
                score = self.sentence_score(sentence, word_freq)
                sentence_scores.append((i, sentence, score))
                print(f"\nå¥å­ {i+1} (åˆ†æ•¸: {score:.4f})")
                print(f"  å…§å®¹: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
            
            # 4. é¸æ“‡æœ€é«˜åˆ†çš„å¥å­
            print("\n" + "="*60)
            print("æ­¥é©Ÿ 3: é¸æ“‡é‡è¦å¥å­")
            print("="*60)
            
            # æ ¹æ“šæ¯”ä¾‹æ±ºå®šæ‘˜è¦å¥å­æ•¸é‡
            summary_length = max(1, int(len(sentences) * ratio))
            print(f"\næ‘˜è¦æ¯”ä¾‹: {ratio:.1%}")
            print(f"é¸æ“‡å¥å­æ•¸: {summary_length}/{len(sentences)}")
            
            # æŒ‰åˆ†æ•¸æ’åºä¸¦é¸æ“‡ top N
            sentence_scores.sort(key=lambda x: x[2], reverse=True)
            selected_sentences = sentence_scores[:summary_length]
            
            print(f"\né¸ä¸­çš„å¥å­:")
            for i, (idx, sent, score) in enumerate(selected_sentences, 1):
                print(f"{i}. å¥å­ {idx+1} (åˆ†æ•¸: {score:.4f})")
                print(f"   {sent}")
            
            # 5. æŒ‰åŸæ–‡é †åºæ’åˆ—
            print("\n" + "="*60)
            print("æ­¥é©Ÿ 4: æŒ‰åŸæ–‡é †åºé‡çµ„æ‘˜è¦")
            print("="*60)
            
            selected_sentences.sort(key=lambda x: x[0])
            summary = ''.join([sent for _, sent, _ in selected_sentences])
            
            print("\nç”Ÿæˆçš„æ‘˜è¦:")
            print("-" * 60)
            print(summary)
            print("-" * 60)
            
            # çµ±è¨ˆè³‡è¨Š
            original_length = len(text)
            summary_length_chars = len(summary)
            compression_ratio = (1 - summary_length_chars / original_length) * 100
            
            print(f"\nå£“ç¸®çµ±è¨ˆ:")
            print(f"  åŸæ–‡å­—æ•¸: {original_length} å­—")
            print(f"  æ‘˜è¦å­—æ•¸: {summary_length_chars} å­—")
            print(f"  å£“ç¸®ç‡: {compression_ratio:.1f}%")
            
            return {
                'summary': summary,
                'selected_sentences': selected_sentences,
                'original_length': original_length,
                'summary_length': summary_length_chars,
                'compression_ratio': compression_ratio
            }
    
    # æ¸¬è©¦æ–‡ç« 
    article = """
äººå·¥æ™ºæ…§ï¼ˆAIï¼‰çš„ç™¼å±•æ­£åœ¨æ·±åˆ»æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ã€‚å¾æ—©ä¸Šèµ·åºŠæ™‚çš„æ™ºæ…§é¬§é˜ï¼Œåˆ°é€šå‹¤æ™‚çš„è·¯ç·šè¦åŠƒï¼Œå†åˆ°å·¥ä½œä¸­çš„å„ç¨®è¼”åŠ©å·¥å…·ï¼ŒAIç„¡è™•ä¸åœ¨ã€‚

åœ¨é†«ç™‚é ˜åŸŸï¼ŒAIå”åŠ©é†«ç”Ÿé€²è¡Œç–¾ç—…è¨ºæ–·ï¼Œæé«˜äº†è¨ºæ–·çš„æº–ç¢ºç‡å’Œæ•ˆç‡ã€‚é€éåˆ†æå¤§é‡çš„é†«ç™‚å½±åƒå’Œç—…æ­·è³‡æ–™ï¼ŒAIèƒ½å¤ ç™¼ç¾äººçœ¼å®¹æ˜“å¿½ç•¥çš„ç´°ç¯€ï¼Œç‚ºæ‚£è€…æä¾›æ›´å¥½çš„æ²»ç™‚æ–¹æ¡ˆã€‚

æ•™è‚²æ–¹é¢ï¼ŒAIå€‹äººåŒ–å­¸ç¿’ç³»çµ±èƒ½å¤ æ ¹æ“šæ¯å€‹å­¸ç”Ÿçš„å­¸ç¿’é€²åº¦å’Œç‰¹é»ï¼Œæä¾›å®¢è£½åŒ–çš„æ•™å­¸å…§å®¹ã€‚é€™ç¨®å› ææ–½æ•™çš„æ–¹å¼ï¼Œè®“å­¸ç¿’è®Šå¾—æ›´åŠ é«˜æ•ˆå’Œæœ‰è¶£ã€‚

ç„¶è€Œï¼ŒAIçš„å¿«é€Ÿç™¼å±•ä¹Ÿå¸¶ä¾†äº†ä¸€äº›æŒ‘æˆ°ã€‚é¦–å…ˆæ˜¯å°±æ¥­å•é¡Œï¼Œè¨±å¤šå‚³çµ±å·¥ä½œå¯èƒ½æœƒè¢«AIå–ä»£ã€‚å…¶æ¬¡æ˜¯éš±ç§å’Œå®‰å…¨å•é¡Œï¼ŒAIç³»çµ±éœ€è¦å¤§é‡æ•¸æ“šä¾†è¨“ç·´ï¼Œå¦‚ä½•ä¿è­·å€‹äººéš±ç§æˆç‚ºé‡è¦è­°é¡Œã€‚æœ€å¾Œæ˜¯å€«ç†å•é¡Œï¼ŒAIçš„æ±ºç­–éç¨‹å¾€å¾€ç¼ºä¹é€æ˜åº¦ï¼Œå¯èƒ½æœƒç”¢ç”Ÿåè¦‹æˆ–æ­§è¦–ã€‚

é¢å°é€™äº›æŒ‘æˆ°ï¼Œæˆ‘å€‘éœ€è¦åœ¨æ¨å‹•AIç™¼å±•çš„åŒæ™‚ï¼Œå»ºç«‹ç›¸æ‡‰çš„æ³•å¾‹æ³•è¦å’Œå€«ç†æº–å‰‡ã€‚åªæœ‰é€™æ¨£ï¼Œæ‰èƒ½ç¢ºä¿AIæŠ€è¡“çœŸæ­£ç‚ºäººé¡ç¦ç¥‰æœå‹™ï¼Œå‰µé€ ä¸€å€‹æ›´ç¾å¥½çš„æœªä¾†ã€‚
"""
    
    print("\næ¸¬è©¦æ–‡ç« :")
    print("="*60)
    print(article.strip())
    print("="*60)
    
    # æ¸¬è©¦æ‘˜è¦ç³»çµ±
    print("\n\né–‹å§‹ç”Ÿæˆæ‘˜è¦...")
    print("="*60)
    
    summarizer = StatisticalSummarizer()
    
    # æ¸¬è©¦ä¸åŒçš„æ‘˜è¦æ¯”ä¾‹
    for ratio in [0.3, 0.5]:
        print("\n\n" + "="*60)
        print(f"æ‘˜è¦æ¯”ä¾‹: {ratio:.0%}")
        print("="*60)
        result = summarizer.summarize(article, ratio=ratio)
    
    # å„²å­˜çµæœ
    print("\n\n" + "="*60)
    print("å„²å­˜çµæœ")
    print("="*60)
    
    os.makedirs('results', exist_ok=True)
    
    all_results = {}
    with open('results/a3_summary_results.txt', 'w', encoding='utf-8') as f:
        f.write("A-3: çµ±è¨ˆå¼è‡ªå‹•æ‘˜è¦çµæœ\n")
        f.write("="*60 + "\n\n")
        
        f.write("åŸæ–‡:\n")
        f.write("-"*60 + "\n")
        f.write(article.strip() + "\n\n")
        
        for ratio in [0.3, 0.5]:
            f.write("\n" + "="*60 + "\n")
            f.write(f"æ‘˜è¦æ¯”ä¾‹: {ratio:.0%}\n")
            f.write("="*60 + "\n\n")
            
            result = summarizer.summarize(article, ratio=ratio)
            all_results[ratio] = result
            
            f.write("æ‘˜è¦:\n")
            f.write("-"*60 + "\n")
            f.write(result['summary'] + "\n\n")
            
            f.write(f"åŸæ–‡å­—æ•¸: {result['original_length']} å­—\n")
            f.write(f"æ‘˜è¦å­—æ•¸: {result['summary_length']} å­—\n")
            f.write(f"å£“ç¸®ç‡: {result['compression_ratio']:.1f}%\n\n")
    
    print("âœ“ æ‘˜è¦çµæœå·²å„²å­˜è‡³: results/a3_summary_results.txt")
    
    # ========== æ–°å¢ï¼šè¦–è¦ºåŒ– ==========
    print("\n" + "="*60)
    print("ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨")
    print("="*60)
    
    # 1. æ‘˜è¦å£“ç¸®ç‡æ¯”è¼ƒåœ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    ratios = [0.3, 0.5]
    compression_ratios = [all_results[r]['compression_ratio'] for r in ratios]
    summary_lengths = [all_results[r]['summary_length'] for r in ratios]
    original_length = all_results[0.3]['original_length']
    
    # å·¦åœ–ï¼šå£“ç¸®ç‡æ¯”è¼ƒ
    bars = axes[0].bar([f'{int(r*100)}%' for r in ratios], compression_ratios, 
                       color=['#FF6B6B', '#4ECDC4'])
    axes[0].set_ylabel('å£“ç¸®ç‡ (%)', fontsize=12)
    axes[0].set_title('ä¸åŒæ‘˜è¦æ¯”ä¾‹çš„å£“ç¸®ç‡', fontsize=14, pad=20)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    for bar, ratio in zip(bars, compression_ratios):
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height,
                    f'{ratio:.1f}%', ha='center', va='bottom', fontsize=12)
    
    # å³åœ–ï¼šå­—æ•¸å°æ¯”
    x = np.arange(len(ratios))
    width = 0.35
    
    bars1 = axes[1].bar(x - width/2, [original_length]*len(ratios), width, 
                        label='åŸæ–‡', color='#95E1D3')
    bars2 = axes[1].bar(x + width/2, summary_lengths, width,
                        label='æ‘˜è¦', color='#F38181')
    
    axes[1].set_ylabel('å­—æ•¸', fontsize=12)
    axes[1].set_title('åŸæ–‡èˆ‡æ‘˜è¦å­—æ•¸å°æ¯”', fontsize=14, pad=20)
    axes[1].set_xticks(x)
    axes[1].set_xticklabels([f'{int(r*100)}%' for r in ratios])
    axes[1].legend()
    axes[1].grid(True, alpha=0.3, axis='y')
    
    # åœ¨é•·æ¢ä¸Šé¡¯ç¤ºæ•¸å€¼
    for bar in bars1:
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=10)
    for bar in bars2:
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('results/a3_summary_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ“ æ‘˜è¦æ¯”è¼ƒåœ–å·²å„²å­˜è‡³: results/a3_summary_comparison.png")
    plt.close()
    
    # 2. å¥å­é‡è¦æ€§åˆ†æ•¸åˆ†å¸ƒ
    # é‡æ–°è¨ˆç®—ä¸€æ¬¡ä»¥ç²å–å¥å­åˆ†æ•¸
    import re
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', article)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    all_words = []
    for sentence in sentences:
        words = [w for w in jieba.cut(sentence) 
                if w not in summarizer.stop_words and len(w) > 1]
        all_words.extend(words)
    
    word_freq = {}
    for word in all_words:
        word_freq[word] = word_freq.get(word, 0) + 1
    
    sentence_scores = []
    for sentence in sentences:
        score = summarizer.sentence_score(sentence, word_freq)
        sentence_scores.append(score)
    
    plt.figure(figsize=(12, 6))
    plt.plot(range(1, len(sentence_scores)+1), sentence_scores, 
             marker='o', linewidth=2, markersize=8, color='steelblue')
    plt.xlabel('å¥å­ç·¨è™Ÿ', fontsize=12)
    plt.ylabel('é‡è¦æ€§åˆ†æ•¸', fontsize=12)
    plt.title('å„å¥å­é‡è¦æ€§åˆ†æ•¸åˆ†å¸ƒ', fontsize=14, pad=20)
    plt.grid(True, alpha=0.3)
    
    # æ¨™è¨˜æœ€é‡è¦çš„å¥å­
    top_indices = np.argsort(sentence_scores)[-3:][::-1]
    for idx in top_indices:
        plt.scatter(idx+1, sentence_scores[idx], color='red', s=200, zorder=5, alpha=0.6)
        plt.annotate(f'Top {list(top_indices).index(idx)+1}', 
                    xy=(idx+1, sentence_scores[idx]),
                    xytext=(10, 10), textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),
                    fontsize=9)
    
    plt.tight_layout()
    plt.savefig('results/a3_sentence_scores.png', dpi=300, bbox_inches='tight')
    print("âœ“ å¥å­åˆ†æ•¸åˆ†å¸ƒåœ–å·²å„²å­˜è‡³: results/a3_sentence_scores.png")
    plt.close()
    
    # 3. è©é›²è¦–è¦ºåŒ– - é¡¯ç¤ºé«˜é »é—œéµè© (é¡å¤–åŠ åˆ†é …ç›®)
    print("\nç”Ÿæˆé—œéµè©è©é›²...")
    
    # å–å¾—æ‰€æœ‰è©çš„é »ç‡
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    word_freq_dict = dict(sorted_words[:50])  # å– Top 50
    
    if word_freq_dict:
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦åœ–ï¼šåŸæ–‡é—œéµè©è©é›²
        wordcloud1 = WordCloud(
            font_path='C:/Windows/Fonts/msjh.ttc',
            width=800, 
            height=600,
            background_color='white',
            colormap='Blues',
            relative_scaling=0.5,
            min_font_size=10
        ).generate_from_frequencies(word_freq_dict)
        
        axes[0].imshow(wordcloud1, interpolation='bilinear')
        axes[0].set_title('åŸæ–‡é—œéµè©è©é›² (ä¾è©é »)', fontsize=14, pad=10)
        axes[0].axis('off')
        
        # å³åœ–ï¼šä½¿ç”¨ä¸åŒé…è‰²çš„è©é›²
        wordcloud2 = WordCloud(
            font_path='C:/Windows/Fonts/msjh.ttc',
            width=800, 
            height=600,
            background_color='#1a1a1a',
            colormap='plasma',
            relative_scaling=0.5,
            min_font_size=10
        ).generate_from_frequencies(word_freq_dict)
        
        axes[1].imshow(wordcloud2, interpolation='bilinear')
        axes[1].set_title('åŸæ–‡é—œéµè©è©é›² (æš—è‰²ä¸»é¡Œ)', fontsize=14, pad=10)
        axes[1].axis('off')
        
        plt.tight_layout()
        plt.savefig('results/a3_wordcloud.png', dpi=300, bbox_inches='tight')
        print("âœ“ é—œéµè©è©é›²å·²å„²å­˜è‡³: results/a3_wordcloud.png")
        plt.close()
    
    print("\n" + "="*60)
    print("âœ“ A-3 å®Œæˆï¼")
    print("="*60)


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main(task=None):
    """
    ä¸»ç¨‹å¼å…¥å£
    task: 'A1', 'A2', 'A3', 'ALL', None
    """
    print("Part A: å‚³çµ±æ–¹æ³•")
    print("="*60)
    
    if task == 'A1':
        run_a1()
    elif task == 'A2':
        run_a2()
    elif task == 'A3':
        run_a3()
    elif task == 'ALL' or task is None:
        run_a1()
        run_a2()
        run_a3()
    else:
        print(f"âš  ç„¡æ•ˆçš„ä»»å‹™: {task}")
        return
    
    print("\n" + "="*60)
    print("ğŸ‰ Part A å®Œæˆï¼")
    print("="*60)
    print("\nçµæœæª”æ¡ˆä½æ–¼ results/ è³‡æ–™å¤¾")


if __name__ == "__main__":
    # æ”¯æ´å‘½ä»¤åˆ—åƒæ•¸
    if len(sys.argv) > 1:
        task = sys.argv[1].upper()
        main(task)
    else:
        main('ALL')